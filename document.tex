\documentclass{article}
\usepackage[margin=1.5in]{geometry} % just to fix margins
\usepackage{setspace}
\usepackage{graphicx} % for graphics (figures)
\usepackage{float} % to keep figures on the same page as its written
\usepackage{titlesec}
\usepackage[parfill]{parskip} % new line between paragraph, no indentation
\usepackage{color} % for color text
\usepackage[font={small,it}]{caption} % side-by-side figures; changing font
\usepackage{subcaption} % for side-by-side figures
\usepackage{url} % for showing raw links

\begin{document}
\newcommand{\figuretitle}[1]{\textbf{#1}\\\vspace{2mm}}

\begin{center}
{\huge Physical Limitations of Hardware Transactional Memory}
\end{center}

\begin{center}
\begin{minipage}{.9\textwidth}
\begin{center}
{\large \textbf{Abstract}}
\end{center}
Hardware transactional memory is a promising solution to the synchronization
problem in multicore software due to its simple semantics and good performance
relative to traditional approaches.
Hardware transactions perform well because the overhead of implementing
transactional memory semantics is incurred at the hardware level, which is much
faster than traditional software implementations.
This reason for their good performance, however, is also the root of their
problems--hardware transactions will abort when the working set exceeds the
capacity of the underlying hardware.
Before we can incorporate this nascent technology into high-performing
concurrent programs, it is necessary to first investigate the physical capacity
constraints of hardware transactions so that we know when they are not a
feasible solution at all.
Our investigation led to revelations about where hardware transactions are
physically implemented on Intel x86 and IBM PowerPC architectures; we anticipate
these findings will increase general understanding of hardware transactional
memory, thus enabling its proliferation into future concurrent programs.
\end{minipage}
\end{center}

\section{Introduction}
As Moore's law for single core microprocessors reaches a plateau, the transition
to multicore microprocessors is in full swing. High-performing concurrent
programs require the effective utilization of these multicore chips, but the
synchronization overhead involved in maintaining memory consistency has been a
major roadblock to building fast concurrent programs.

With the advent of Intel and IBM microprocessors that support \textit{hardware
transactional memory}, a new contender to solving the synchronization problem is
on the horizon. Hardware transactions are faster than traditional
\textit{coarse-grained locks} and \textit{software transactions}, yet they do
not suffer from the same performance-complexity tradeoff that makes
\textit{fine-grained locks} and atomic CPU instructions, like
\textit{compare-and-swap}, less desirable. Before we can even hope to
effectively use this new technology, however, we must first learn more about
what limitations the underlying hardware imposes on hardware transactions.

Implementing transactions involves the logical maintenance of read sets, the set
of memory locations that are read within a transaction, and write sets, the set
of memory locations that are written within a transaction. When transactional
blocks execute, local reads and writes to memory are tracked and recorded in
corresponding read sets and write sets. Upon completion of a transactional
block, the memory state is validated for consistency before the transaction
\textit{commits} and the modifications to memory are installed.

Concurrently running transactions may \textit{abort} due to a conflict when an
inconsistent memory state is detected, such as when one or many memory locations
in one thread's write set intersects one or many memory locations in another
thread's read set or write set. In addition to \textit{conflict aborts},
hardware transactions specifically suffer from \textit{capacity aborts} when the
read set or write set is too large to fit into the underlying hardware.

We show in this paper the results of experiments we ran to determine where the
read sets and write sets are implemented in the Intel x86 and IBM PowerPC
architectures. The focus of our experiments is to explore scenarios where these
capacity aborts are inevitable so that we know when hardware transactions are
not a feasible synchronization solution at all, even in the case of no
contention. These contributions will provide important insights to the limits of
hardware transactional memory and better enable its effective utilization in
future multicore programs.

\section{Experimental Setup}
\subsection{Hardware Specifications}
The results from our experiments should only be fully accepted with respect to
the microprocessors we specify in this section, although the conclusions will
generally apply to different generations of the hardware.

The Intel experimental machine contains a Haswell i7-4770 processor with

\begin{spacing}{0}
\begin{itemize}
\item 4 3.40GHz cores; 4 total hardware threads
\item 64 byte wide cache lines
\item 8mB shared 16-way \textit{L3} cache
\item 32kB per-core 8-way \textit{L1} cache
\end{itemize}

The IBM experimental machine contains a Power8 processor with
\begin{itemize}
\item 10 3.425GHz cores; 80 total hardware threads
\item 128 byte wide cache lines
\item 80mB shared 8-way \textit{L3} cache (roughly 8mB per-core)
\item 64kB per-core 8-way \textit{L1} cache
%http://www.7-cpu.com/cpu/Power8.html
%https://books.google.com/books?id=LIhFBAAAQBAJ&pg=PA36&lpg=PA36&dq=ibm+power8+cache+associativity&source=bl&ots=h7WlkWlmYD&sig=w0gxYk9H2-BkKEeUgnnLgS0oWC0&hl=en&sa=X&ei=ibm1VJznOYOigwSctoKoAQ&ved=0CEgQ6AEwBQ#v=onepage&q=ibm%20power8%20cache%20associativity&f=false
\end{itemize}
\end{spacing}

\subsection{Hardware Transactional Memory Interface}
All experiments are written in C and compiled with GCC, optimization level
\textit{-O0}. Our experiments use the GCC hardware transactional memory
intrinsics interface.

\section{Capacity Constraints}
There are physical limitations to the size of hardware transactions that are
governed by how they are implemented in hardware. These capacity constraints
determine when a transaction will inevitably abort, even in the case of zero
contention. It is thus critical to shed light on the hardware implementation of
transactions in order to know when they are not a feasible synchronization
solution at all.

Hardware transactional memory leverages existing caching protocols to maintain
read sets and write sets. As such, we devised an array access experiment to
measure the maximum cache line capacity of sequential read-only and write-only
hardware transactions. We also experimented with strided memory access patterns
to see how different stride amounts affect maximum possible transaction sizes.
With knowledge of the maximum sequential access capacity and also the maximum
strided access capacity, we can draw conclusions about where in the caching
architecture the read sets and write sets are implemented. We report these
results for both the Intel and IBM machines.

\subsection*{Intel}

Figure~\ref{fig:wttm_capacity_read_intel} shows the result of a sequential
read-only access experiment. The data points in the graphs represent the success
probability of the transaction with respect to the number of cache lines read.
The results indicate that a single transaction can reliably read 
around 75000 cache lines and still commit..

Figure~\ref{fig:wttm_stride_read_intel} shows the result of a strided read-only
access experiment. The stride amount is the number of cache lines stepped over
per access. Each data point in the graph represents the maximum number of cache
lines that can be reliably read with respect to the stride amount. For example,
the third data point in the graph indicates that when the stride amount is
$2^2=4$ cache lines stepped over per access, the transaction can reliably read
$2^{14}=16384$ cache lines and commit. The significance of this graph is that
the number of cache lines that can be read in a single transaction is generally
halved as we double the stride amount, presumably because the access pattern
keeps hitting the same few cache sets while completely skipping over other sets.
It is important to note that the plot plateaus at $2^4=16$ cache lines.

Figure~\ref{fig:wttm_capacity_read_intel} and
Figure~\ref{fig:wttm_stride_read_intel} show experimental results that indicate
that the read set is very closely related to, if not directly stored inside, the
\textit{L3} cache. The \textit{L3} cache of this Intel machine has a maximum
capacity of $2^{17}$ cache lines, which explains why read-only transactions
cannot fit much more than $2^{16}=65536$ cache lines because it is impossible
for the whole read set to fit perfectly into the \textit{L3}. The minimum of 16
cache lines readable when the stride amounts are large enough to consecutively
hit the same cache set further supports the notion that the read set is
maintained through the \textit{L3} because this value exactly coincides with the
set associativity of the \textit{L3}, which is 16 cache lines.

Figure~\ref{fig:wttm_capacity_write_intel} illustrates the result of an
identical array access experiment, except now the transactions are write-only
instead of read-only. A single write-only transaction can reliably commit about
400 cache lines.

Figure~\ref{fig:wttm_stride_write_intel} illustrates that the number of cache
lines that can be written in a single transaction is also generally halved as we
double the stride amount, but even as we increase the stride amount
significantly, the number of cache lines that a transaction can reliably write
to does not fall below 8.

The evidence from Figure~\ref{fig:wttm_capacity_write_intel} and
Figure~\ref{fig:wttm_stride_write_intel} suggests that the write set is very
closely related to, if not directly stored inside, the \textit{L1} cache. The
size of the \textit{L1} cache is 512 cache lines, and it makes sense that the
write set can only reach about 400 cache lines because it cannot possibly fit
exactly into the \textit{L1}. The associativity of the \textit{L1} is 8, and
this number coincides with the number of cache lines that can still be written
to for large strided access patterns that hit the same cache set.

\begin{figure}[]%[ht!]
\centering
\figuretitle{Cache Lines Read vs Success Probability}
\includegraphics[width=\linewidth]{images/wttm_capacity_read_intel}
\caption{The read set maximum sequential access capacity on the Intel machine is
around 75,000 cache lines}
\label{fig:wttm_capacity_read_intel}
\end{figure}

\begin{figure}[]%[ht!]
\centering
\figuretitle{Stride Amount vs Cache Lines Readable}
\includegraphics[width=\linewidth]{images/wttm_stride_read_intel}
\caption{Doubling the stride amount halves the size of successful read-only
transactions to a minimum of 16 on the Intel machine}
\label{fig:wttm_stride_read_intel}
\end{figure}

\begin{figure}[]%[ht!]
\centering
\figuretitle{Cache Lines Written vs Success Probability}
\includegraphics[width=\linewidth]{images/wttm_capacity_write_intel}
\caption{The write set maximum sequential access capacity on the Intel machine is
around 400 cache lines}
\label{fig:wttm_capacity_write_intel}
\end{figure}

\begin{figure}[]%[ht!]
\centering
\figuretitle{Stride Amount vs Cache Lines Writable}
\includegraphics[width=100mm]{images/wttm_stride_write_intel}
\caption{Doubling the stride amount halves the size of successful write-only
transactions to a minimum of 8 on the Intel machine}
\label{fig:wttm_stride_write_intel}
\end{figure}

Judging from the results of our capacity constraint experiments, it is likely
the case that the read set is implemented through the \textit{L3} cache and the
write set is implemented through the \textit{L1} cache on the experimental Intel
machine, and even more generally on other Intel x86 microprocessors that support
hardware transactional memory.

\subsection*{IBM}

Figure~\ref{fig:wttm_capacity_readwrite_ibm} shows that the maximum number of
cache lines that can be read or written in a single hardware transaction on the
IBM machine is very clearly 63 cache lines.

The results of our strided access experiment for both read-only and write-only
transactions appear to be identical in
Figure~\ref{fig:wttm_stride_readwrite_ibm}; the doubling-stride-amount
$\rightarrow$ halving-transaction-size effect is still observed and a minimum of
16 cache lines can be read or written in a single transaction.

The maximum observed hardware transaction size of 63 on this machine is far too
small to be attributed to even the \textit{L1} cache, which holds 512 cache
lines. The identical measurements between the read set and write set found in
Figures~\ref{fig:wttm_capacity_readwrite_ibm}
and~\ref{fig:wttm_stride_readwrite_ibm} suggest that there is little distinction
between the handling of reads and writes in a transaction on the IBM machine.
From these results, we conclude that there are dedicated caches for transactions
on the IBM machine and, more generally, for the PowerPC architecture. The
dedicated caches likely each have 4 cache sets and a set associativity of 16,
for a total of 64 cache lines.

\begin{figure}[]%[ht!]
\centering
\figuretitle{Cache Lines Read/Written vs Success Probability}
\includegraphics[width=\linewidth]{images/wttm_capacity_readwrite_ibm}
\caption{The read set and write set maximum sequential access capacity on the
IBM machine is 63 cache lines}
\label{fig:wttm_capacity_readwrite_ibm}
\end{figure}

\begin{figure}[]%[ht!]
\centering
\figuretitle{Stride Amount vs Cache Lines Readable/Writeable}
\includegraphics[width=100mm]{images/wttm_stride_readwrite_ibm}
\caption{Doubling the stride amount halves the size of successful read-only and
write-only transactions to a minimum of 16 on the IBM machine}
\label{fig:wttm_stride_readwrite_ibm}
\end{figure}

\begin{figure}[]%[ht!]
\centering
\figuretitle{Num Threads vs Committed Transactions}
\includegraphics[width=\linewidth]{images/wttm_core_or_thread_ibm}
\caption{Aggregate committed transactions increases with the number of threads
while average committed transactions remains constant, suggesting that there is
a dedicated cache per core on the IBM machine}
\label{fig:wttm_core_or_thread_ibm}
\end{figure}

A natural next question is whether this IBM machine has 10 dedicated caches that
are spread across each core, or if there are 80 dedicated caches that are spread
across each hardware thread. To determine the difference, we experimented and
measured the number of successful write-only transactions that concurrently
running threads were able to complete. Each thread makes 10000 transaction
attempts to write 40 thread-local padded cache lines and then commit. The
transaction size of 40 cache lines is designed to sufficiently fill up the
dedicated caches per transaction to induce capacity aborts in the case of shared
caches.

We see in Figure~\ref{fig:wttm_core_or_thread_ibm} compelling reason to believe that
there are 80 dedicated caches, one for each hardware thread. Each spawned
software thread is pinned to a unique hardware thread in round robin fashion
such that the distribution is even across the 10 cores. If all 8 of the
hardware threads on a single core share a single dedicated cache, we would
expect to see sublinear speedup, perhaps even degraded performance, as we spawn
more running threads and assign them to the same core. Instead, we observe a
linear increase in the aggregate number of successfully committed transactions,
while the average per-thread number of successful transactions is constant.
Although the general 45\% success rate suggests some level of contention between
the running threads, it is most likely not due to per-core sharing of a
dedicated cache.

\section{Discussion/Implications}
*Now that the results are presented, I feel the need to discuss their
implications, like my anecdotal encounter with the failed program on the IBM
machine. Does this warrant its own section, or should we cram it into the
conclusion?*

\section{Conclusion}
Hardware transactional memory is an exciting new solution to the synchronization
problem in multicore software. Before we can effectively use this new
technology, we must first understand the physical limits of hardware
transactions that are inherent to the Intel and IBM microprocessors on which
they are implemented. Our capacity constraint benchmarks revealed that the read
sets are implemented through the \textit{L3} and the write sets are implemented
through the \textit{L1} on the Intel machine; on the other hand, there is a
per-core, 4-set, 16-way dedicated cache that maintains the read and write sets
on the IBM machine. *INSERT SOME DISCUSSION/IMPLICATION/REVELATION, like how IBM
is not useful for sufficiently large transactions*. We anticipate that these
findings will move us in the right direction to better understanding hardware
transactional memory, ultimately enabling its proliferation into future
concurrent programs.

\section{References}

\end{document}



















